{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzWsZQRLKDVm",
        "outputId": "258102d2-c127-49a5-9d8f-a96331fb7638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec  2 19:49:48 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 552.46                 Driver Version: 552.46         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   43C    P8              1W /   64W |     113MiB /   8188MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A     27808    C+G   ...5\\extracted\\runtime\\WeChatAppEx.exe      N/A      |\n",
            "|    0   N/A  N/A     34096    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "# check which GPU we are using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PA0nJ-s0VT_A"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxGBqxa2Wi4m"
      },
      "source": [
        "### Purpose of accelerate:\n",
        "1. Ease of Multi-Device Training: whether you're using multiple GPUs or TPUs, accelerate makes it easier to scale your model across devices with minimal code changes\n",
        "\n",
        "2. Mixed Precision: It allows models to be trained using mixed precision, which can speed up traning and reduce memory usage\n",
        "3. Zero Redundancy Optimizer (ZeRO): helps manage large models by efficiently splitting the model across multiple devices\n",
        "4. Offload to CPU/SSD: Useful for large models that may not fit entirely into GPU memory, by allowing parts of the model or optimizer to be offloaded to CPU or even SSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILGrRgYFWIPg",
        "outputId": "8c8500f7-4a89-4c7d-dccc-cec6932a780f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (0.26.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (2.5.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Found existing installation: transformers 4.46.3\n",
            "Uninstalling transformers-4.46.3:\n",
            "  Successfully uninstalled transformers-4.46.3\n",
            "Found existing installation: accelerate 1.1.1\n",
            "Uninstalling accelerate-1.1.1:\n",
            "  Successfully uninstalled accelerate-1.1.1\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from accelerate) (2.5.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "Using cached accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
            "Installing collected packages: accelerate, transformers\n",
            "Successfully installed accelerate-1.1.1 transformers-4.46.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade accelerate \n",
        "#accelerate is used to assign jobs better than the GPU\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgetsNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
            "  Using cached widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
            "  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: decorator in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: stack_data in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
            "Using cached widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
          ]
        }
      ],
      "source": [
        "pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCOClmAOYuPY",
        "outputId": "f902dfe7-7d23-47bf-dd60-ee9da1f3c7b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ericz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "#from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# AutoTokenizer converts texts into tokens for every each model that is compatible to the model and converts texts into tokens\n",
        "# AutoModelForSeq2SeqLM - used for loading the model\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzp1gxVjd5j8"
      },
      "source": [
        "### Basic Functionality of Huggingface Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "PJkfYPymcu2j",
        "outputId": "747ee494-0f37-4030-8e73-fbf8b40bc4c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ericz\\.cache\\huggingface\\hub\\models--google--pegasus-xsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"California's largest electricity provider has turned off power to hundreds of thousands of customers.\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        ")\n",
        "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"])\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dXrExeUceAlo",
        "outputId": "2db9e861-28e9-4b74-a715-051d1212b09d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpV53jjeQk5"
      },
      "source": [
        "### Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFtB3DoeW4D",
        "outputId": "b22302f1-d913-48aa-eedc-a6db5ec278dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model) #load a tokenizer, convert text into tokens\n",
        "\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model).to(device) #loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av_RdT9y1MMh",
        "outputId": "d61bce25-cd82-4ec5-f8ec-a50de176af51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py7zr in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (0.22.0)\n",
            "Requirement already satisfied: texttable in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (3.21.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (0.16.2)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (1.0.2)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (1.0.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\ericz\\textsummarizer\\venv\\lib\\site-packages (from py7zr) (6.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-75xz6F4e5_s",
        "outputId": "d29c3040-ff25-4e6d-bccb-3314d15272a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n",
            "'unzip' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n"
          ]
        }
      ],
      "source": [
        "# download and unzip data\n",
        "!wget https://github.com/ez-anthro-tech-design/datasets/blob/main/summarizer_data.zip\n",
        "!unzip summarizer_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL4NbjZdSzoU",
        "outputId": "39a26b9f-261e-4733-df41-5fe24b9814b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 14732 examples [00:00, 42276.09 examples/s]\n",
            "Generating test split: 819 examples [00:00, 23190.63 examples/s]\n",
            "Generating validation split: 818 examples [00:00, 27217.16 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'summary', 'dialogue'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'summary', 'dialogue'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'summary', 'dialogue'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the data from JSON files\n",
        "dataset_samsum = load_dataset('json', data_files={'train': 'train.json', 'test': 'test.json', 'validation': 'val.json'})\n",
        "\n",
        "# Print the dataset\n",
        "dataset_samsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIF2gu-pWEfD",
        "outputId": "699ef4b9-ca83-4b44-aab3-bc38a0828f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split lengths: [14732, 819, 818]\n",
            "Features: ['id', 'summary', 'dialogue']\n",
            "\n",
            "Dialogue:\n",
            "Gloria: This exam is a bit of a lottery in fact\n",
            "Gloria: You can't really get prepared, it's all about experience\n",
            "Emma: But there are some rules and some typical texts right?\n",
            "Gloria: You can see some texts from previous years\n",
            "Gloria: <file_other>\n",
            "Emma: Wow that's very useful\n",
            "Emma: I have never seen this site\n",
            "Gloria: Yes it's very good\n",
            "Gloria: Actually it's good to read all the texts because you will see that some phrases repeat very often\n",
            "Emma: How much time do you have for all 4 parts?\n",
            "Gloria: 4 hours\n",
            "Emma: Is it enough?\n",
            "Gloria: Well it has to be\n",
            "Gloria: Would be perfect to have 2 more hours... But on the other hand it would be really exhausting\n",
            "Emma: 4 hours and no breaks?\n",
            "Gloria: No breaks :/ So it's really important to be really focused and try to write as fast as you can\n",
            "Gloria: And read it carefully and correct during the last hour\n",
            "Emma: I'm going to read everything from that website, it's great\n",
            "\n",
            "Summary:\n",
            "Gloria has an exam soon. It lasts 4 hours. Emma sent her a link to a website with some texts from previous years so that she can prepare for the exam better.\n"
          ]
        }
      ],
      "source": [
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum] #how much is the data length of the train data, the test data, and the validation data\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][21][\"dialogue\"])\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][21][\"summary\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcXOkh9efrQS"
      },
      "source": [
        "### Preparing Data For Training For Sequence To Sequence Model\n",
        "\n",
        "{\n",
        "  'dialogue': \"Hi! How are you?\",\n",
        "  \"summary\": \"The speaker is asking how the other person is.\"\n",
        "\n",
        "}\n",
        "-> converted to\n",
        "{\n",
        "  'input_ids': [123, 456, 789, ...], # Token IDs for the dialogue\n",
        "  'attention_mask': [1, 1, 1, ...], # Attention mask for the input\n",
        "  'labels': [321, 654, 987, ...], # Token IDs for the summary (target)\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RlNSa5OPfX24"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "      target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "      'input_ids' : input_encodings['input_ids'],\n",
        "      'attention_mask': input_encodings['attention_mask'],\n",
        "      'labels': target_encodings['input_ids']\n",
        "\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0c9790d45ac44c338f7e31cda1039a2f",
            "742515997a964923b3326e573baf635b",
            "4a9454f2368949c7831032e1e9482c95",
            "f524c0f3d6254008b26b64e5724e9e79",
            "472dca91dcda4879a01259a7891e5fd3",
            "f6b9ae3c8a8c47589edc7a051d1aa9bf",
            "d203f78b4f2945c8b5a47e8aded8e7b1",
            "ea179ea139ae4b27af2959cac28e0214",
            "2e72a093d22942b5bbd1e0498a69b3d3",
            "ff844b6fc7e6496db17e19b3e544c18b",
            "cabb53e2822e4dcba549cad1209d80b1"
          ]
        },
        "id": "JuXD-s6wip-u",
        "outputId": "e96be117-d2f2-44f5-c920-2c1a1c0dbf9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]c:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 14732/14732 [00:03<00:00, 4417.17 examples/s]\n",
            "Map: 100%|██████████| 819/819 [00:00<00:00, 1907.54 examples/s]\n",
            "Map: 100%|██████████| 818/818 [00:00<00:00, 4165.57 examples/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zacepm6_jEcZ",
        "outputId": "9f6ea87b-8a9d-4220-81bc-0b7950d9e0ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'summary', 'dialogue', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 14732\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum_pt[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQA-ZcdtkND6",
        "outputId": "cd95110a-e7a5-4696-8ef7-ebbe5f8cf0d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'summary', 'dialogue', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 819\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum_pt[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srWvjMx8k2j-"
      },
      "source": [
        "DataCollatorForSeq2Seq is a special data collator designed for sequence-to-sequence models (e.g. Pegasus, T5, BART) that helps in preparing batches of data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "l6iGlKmbkSbo"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq # makes sure whatever data we have we convert that into batch so that it can be provided to the model for training\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RJ3QG0TKoB3R"
      },
      "outputs": [],
      "source": [
        "# fine tuning data\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args= TrainingArguments(\n",
        "    output_dir='pegasus-samsum', num_train_epochs=5, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    eval_strategy='steps', eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUtYMmTipqia",
        "outputId": "5d0a030c-7c97-4261-ca65-e9186ab6513c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ericz\\AppData\\Local\\Temp\\ipykernel_36040\\4066997412.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(model=model_pegasus, args=trainer_args,\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
        "          tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "          train_dataset=dataset_samsum_pt[\"test\"], #test data is smaller, for quicker processing\n",
        "          eval_dataset=dataset_samsum_pt[\"validation\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "SxxhqCkpqITS",
        "outputId": "5fa9fc43-ca6e-4e78-8c5b-db88c04d9ade"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 10/255 [15:26<14:33:40, 213.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1101, 'grad_norm': 511.05169677734375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 20/255 [18:29<1:39:51, 25.50s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0467, 'grad_norm': 253.41697692871094, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 30/255 [23:41<1:56:24, 31.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1613, 'grad_norm': 164.25572204589844, 'learning_rate': 3e-06, 'epoch': 0.59}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 38/255 [27:29<1:43:27, 28.60s/it]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\accelerate\\accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ericz\\TextSummarizer\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQU0xwz4rBEj"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "### 1st[1,2,3,4,5,6] -> [1,2,3][4,5,6]\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elemnts,\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=device,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                  attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                  length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we docde the generated texts,\n",
        "        # replace the token, and add the decoded texts with the references to the metric\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUrWms1Kr7gU",
        "outputId": "634fb4c8-d421-45f8-a427-90b8fc845e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "ec190c2bfa8a436588e8c0aba4aa42a8",
            "09113dadc03143deb5d182c1ec128206",
            "f64c2d717b0a4e00aca1cc8f12ee3b89",
            "da62672b54c84267acb4ca17a8cd2e87",
            "7fb9f38d4b15448db26142942e110207",
            "bd7621e37a2644c9afaeaefb130aa901",
            "7b7ec5ee04474c5c83bcd232b56c973d",
            "a661de7d0e8f4af7a53dbd9b33e7e2af",
            "c12d90b11b7344f6908e92c74db9a135",
            "93e601308baa4122bb40cbfb85e733c7",
            "cd506262130b4fb484faf6b4f5fff530"
          ]
        },
        "id": "LjH96gaMuasG",
        "outputId": "ac1324ba-ab9c-45a2-f19d-ca8b8e755cac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec190c2bfa8a436588e8c0aba4aa42a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_metric = evaluate.load('rouge')\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "#rouge_metric = load_meric('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO6rOfMrt_C-"
      },
      "outputs": [],
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")\n",
        "\n",
        "# Directly use the scores without accessing fmeasure of mid\n",
        "rouge_dict = {rn: score [rn] for rn in rouge_names}\n",
        "\n",
        "# Convert the dictionary to a DataFrame for easy visualization\n",
        "import pandas as pd\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'])\n",
        "\n",
        "\n",
        "# score closer to 1 the better, rouge 1 = perfectly match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpKPDjxMvt_J"
      },
      "source": [
        "### Interpreting Good vs Bad ROUGE scores\n",
        "1. Scores close to 1: This indicates a strong overlap between the generated summary and the reference summary, which is desirable in summarization tasks. For example, an F-1 score of 0.7 or higher across metrics is generally considered good.\n",
        "2. Scores between 0.5 and 0.7: Indicates moderate overlap. The summary might be capturing key points but is likely missing some structure or important information\n",
        "3. Scores below 0.5: Suggest a poor match between the generated and reference summaries. The model might be generating irrelevant or incomplete summaries that don't capture the key ideas well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-HusJHKmefv"
      },
      "outputs": [],
      "source": [
        "## save model\n",
        "\n",
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc2HZyWKmhrJ"
      },
      "outputs": [],
      "source": [
        "## save tokenizer\n",
        "tokenzier.save_pretrained(\"tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq3GNCPdmk76"
      },
      "outputs": [],
      "source": [
        "# Load from modelcheckpoints and checkpoints\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilk9TPflmukG"
      },
      "outputs": [],
      "source": [
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128} # some patterns to load based on,\n",
        "# max_length = how many words the summarizer will be,\n",
        "# length_penalty = if the value is greater than one\n",
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
        "# creating the pipeline based on the model\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nRefernce Summary:\")\n",
        "print(reference)\n",
        "# true data\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\n",
        "# the output generated from the model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09113dadc03143deb5d182c1ec128206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd7621e37a2644c9afaeaefb130aa901",
            "placeholder": "​",
            "style": "IPY_MODEL_7b7ec5ee04474c5c83bcd232b56c973d",
            "value": "Downloading builder script: 100%"
          }
        },
        "0c9790d45ac44c338f7e31cda1039a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_742515997a964923b3326e573baf635b",
              "IPY_MODEL_4a9454f2368949c7831032e1e9482c95",
              "IPY_MODEL_f524c0f3d6254008b26b64e5724e9e79"
            ],
            "layout": "IPY_MODEL_472dca91dcda4879a01259a7891e5fd3"
          }
        },
        "2e72a093d22942b5bbd1e0498a69b3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "472dca91dcda4879a01259a7891e5fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a9454f2368949c7831032e1e9482c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea179ea139ae4b27af2959cac28e0214",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e72a093d22942b5bbd1e0498a69b3d3",
            "value": 819
          }
        },
        "742515997a964923b3326e573baf635b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b9ae3c8a8c47589edc7a051d1aa9bf",
            "placeholder": "​",
            "style": "IPY_MODEL_d203f78b4f2945c8b5a47e8aded8e7b1",
            "value": "Map: 100%"
          }
        },
        "7b7ec5ee04474c5c83bcd232b56c973d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fb9f38d4b15448db26142942e110207": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e601308baa4122bb40cbfb85e733c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a661de7d0e8f4af7a53dbd9b33e7e2af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7621e37a2644c9afaeaefb130aa901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12d90b11b7344f6908e92c74db9a135": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cabb53e2822e4dcba549cad1209d80b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd506262130b4fb484faf6b4f5fff530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d203f78b4f2945c8b5a47e8aded8e7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da62672b54c84267acb4ca17a8cd2e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93e601308baa4122bb40cbfb85e733c7",
            "placeholder": "​",
            "style": "IPY_MODEL_cd506262130b4fb484faf6b4f5fff530",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 235kB/s]"
          }
        },
        "ea179ea139ae4b27af2959cac28e0214": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec190c2bfa8a436588e8c0aba4aa42a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09113dadc03143deb5d182c1ec128206",
              "IPY_MODEL_f64c2d717b0a4e00aca1cc8f12ee3b89",
              "IPY_MODEL_da62672b54c84267acb4ca17a8cd2e87"
            ],
            "layout": "IPY_MODEL_7fb9f38d4b15448db26142942e110207"
          }
        },
        "f524c0f3d6254008b26b64e5724e9e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff844b6fc7e6496db17e19b3e544c18b",
            "placeholder": "​",
            "style": "IPY_MODEL_cabb53e2822e4dcba549cad1209d80b1",
            "value": " 819/819 [00:01&lt;00:00, 553.68 examples/s]"
          }
        },
        "f64c2d717b0a4e00aca1cc8f12ee3b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a661de7d0e8f4af7a53dbd9b33e7e2af",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c12d90b11b7344f6908e92c74db9a135",
            "value": 6270
          }
        },
        "f6b9ae3c8a8c47589edc7a051d1aa9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff844b6fc7e6496db17e19b3e544c18b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
